<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://kasjfksj.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kasjfksj.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-04T17:45:58-07:00</updated><id>https://kasjfksj.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">First year conclusion</title><link href="https://kasjfksj.github.io/blog/2024/First-year/" rel="alternate" type="text/html" title="First year conclusion"/><published>2024-09-16T09:15:09-07:00</published><updated>2024-09-16T09:15:09-07:00</updated><id>https://kasjfksj.github.io/blog/2024/First-year</id><content type="html" xml:base="https://kasjfksj.github.io/blog/2024/First-year/"><![CDATA[<p>I would say that my first year experience in UCI is somewhat successful and I achieved something.</p> <p>Firstly, I participated in an AI Innovation Challenge last year, which gave me the chance to collaborate with other students passionate about AI. Our project involved developing an AI chatbot, and I was particularly excited about our first meeting where we brainstormed various features and discussed technologies like LLMs and databases. Despite our enthusiasm, we struggled with a lack of clear goals, which led us to continuously add new features every week. A week before the deadline, we still hadn’t completed a single feature. It was at this moment that I stood up and held a meeting. I told teammates that we should have a determined goal and we should focus on developing a feature this week. This shift in approach allowed us to complete the project, but we only received a participation award as our single feature was not enough to advance in the competition. From this experience I gained friendship with a student in senior, and later he would introduce other seniors who have passions on AI as well. I also gained lessons of the importance of a clear goal.</p> <p>Secondly, I attended a UCB course and had a wonderful experience, as I mentioned in my previous blog. I really enjoyed campus life at UCB—I visited the campus, made friends, and achieved an impressive A+ in my course. Since my goal was to use this UCB course to replace ICS 31-32 at UCI, I believe this experience was quite successful.</p> <p>Thirdly, I took the GRE for the first time and scored 334 with a 3 on the writing section. I was really pleased with the result, especially since it was my first attempt. I expanded my vocabulary significantly during the preparation for GRE, which helped me in Writing 60. I was able to use euphuism to enhance my writing, ultimately earning an A in that course—something I never thought was possible!</p>]]></content><author><name></name></author><category term="classes"/><category term="experience"/><summary type="html"><![CDATA[I would say that my first year experience in UCI is somewhat successful and I achieved something.]]></summary></entry><entry><title type="html">Grokking - a possible way to achieve AGI</title><link href="https://kasjfksj.github.io/blog/2024/Grokking/" rel="alternate" type="text/html" title="Grokking - a possible way to achieve AGI"/><published>2024-08-30T09:15:09-07:00</published><updated>2024-08-30T09:15:09-07:00</updated><id>https://kasjfksj.github.io/blog/2024/Grokking</id><content type="html" xml:base="https://kasjfksj.github.io/blog/2024/Grokking/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Bigger is better. This phrase is so simple to be true, but it has been testified on AI. With the advent of ChatGPT, people realized that improving model’s performance can be so simple as increasing the size of model and dataset. The scaling laws - bigger and better - for model’s parameter and datasets have been studied thoroughly. However, there are few people studying how increasing training period can affect the model’s performance. Traditionally, if the model is trained for too long, it will enter what’s known as overfitting, a phenomenon that the train accuracy stays high while the validation accuracy drops. We would assume that this model failed to generalize over long period of time due to the poor performance on dataset the model has never seen before.</p> <h2 id="grokking">Grokking</h2> <p>But what happens if we keep on training after overfitting period? Will the test accuracy stays low? This is what researchers in Gork paper answer. They first have a small algorithmically generated datasets: binary operation of division mod 97. Then, they train a neural network on it, recording train accuracy and validation accuracy at each step. They found out that train accuracy reaches near 100% at around \(10^3\) steps. Validation accuracy, however, takes \(10^5\) steps to suddenly achieve near 100%. There’s no steady progress of val acc before \(10^5\).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/35561725154306_.pic.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>There’s a limitation in this paper. They only produce this phenomenon on simple dataset generated by modular function, which has strong data pattern. It’s not sure whether this phenomenon works on much more complex dataset, such as image and language. Even if it does work on complex dataset, it will surely take exceedingly long time to train and finally achieve grooking effect.</p> <p>However, this is quite an interesting phenomenon and needs to be studied. If this grokking effect can happen in every model and every training process, it means that we don’t need additional tricks to maintain validation accuracy of the model. We just need the model to train for a long time, and it will generalize automatically. In that case, we are a step closer to AGI.</p> <h2 id="grokking-fast---low-frequent-gradient">Grokking fast - low frequent gradient</h2> <p>As mentioned before, reaching grokking phenomenon can be time-consuming. For a simple dataset produced by modular, it takes \(1000\times\) for val acc to rise compared to train acc. For a larger complex dataset, the time may take too long to observe any rice of val accuracy.</p> <p>The authors in Grokfast aimed to solve this problem by proposing to amplify slow gradients to accelerate grokking effect. They assumed that grokking effect was due to the slow-varying component of the gradient that results in long time grokking. They treat gradient as a discrete signal and apply low-pass filter to acquire gradients with low frequency, i.e. slow-varying. After that, use a hyper parameter to augment slow gradient and add it to total gradient. Overall, this paper proposes an optimization step to help the model achieve grokking faster.</p> <p>There are of course some advantage and disadvantage of this method, which I will discuss later in the blog.</p> <h2 id="conclusion">Conclusion</h2> <p>I believe grokking has the potential to achieve the same impact as the scaling laws. The latter emphasizes on the largeness on model while the first emphasizes on largeness of training time. Combining those two laws, I believe large language model can go further in generalization.</p>]]></content><author><name></name></author><category term="encoding"/><category term="method"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Grokking - a possible way to achieve AGI</title><link href="https://kasjfksj.github.io/blog/2024/Analysis-Grokfast/" rel="alternate" type="text/html" title="Grokking - a possible way to achieve AGI"/><published>2024-08-30T09:15:09-07:00</published><updated>2024-08-30T09:15:09-07:00</updated><id>https://kasjfksj.github.io/blog/2024/Analysis-Grokfast</id><content type="html" xml:base="https://kasjfksj.github.io/blog/2024/Analysis-Grokfast/"><![CDATA[<p>As mentioned in previous blog, grokking is a phenomenon that extremely long training time leads to a sharp incease in val accuracy. Some researchers try to find a way to speed up the grokking phenomenon. GrokFast proposed a new optimizer to boost slow-varying component of the gradient as they hypothesized that it was a contributing factor to Grokking phenomenon.</p> <p>Initially, I believe that GrokFast could be significant and was puzzled when there was only 1 citation currently. However, further testing on GrokFast shows that it was not capable enough to speed up generalization on test dataset. Default setup, using 3 layer and 200 hidden layer size can achieve good result on fast grokking, but other setup will result in much slower learning procedure, resulting much later increase in train accuracy and val accuracy. The test play results are presented below</p> <h1 id="grokfast---testing">GrokFast - testing</h1> <p>I first test the default setup mentioned above. The result is quiet amazing, as we can see the much earlier increase in val acc.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/grok_fast/grok_fast_none.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> No GrokFast </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/grok_fast/grokfast_em.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> With GrokFast </div> <p>Later, I changed some parameters in GrokFast to test its general capability over all kinds of network. However, the result was not satsifying.</p> <h1 id="grokfast---effect-of-network-depth">GrokFast - effect of network depth</h1> <p>First, I changed the number of layers to 4 and 5 and tested the performance. While train acc and val acc increased simultaneously, the number of steps required to significantly increase train acc and val acc rised from \(10^3\) to around \(10^4\). This phenomenon was quite puzzling, and I speculated that it was due to the magnifying low-varying gradient component.</p> <p>However, there’s still some interesting results of the experiment.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/grok_fast/grokfast_4layer.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 4layer Grokfast experienced sharper increase than 3layer Grokfast </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/grok_fast/grokfast_5layer.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 5layer Grokfast experienced sharper increase than 4layer Grokfast at $$10^4$$ steps </div> <p>We can see that the network experienced sharper increase at \(10^4\) steps.</p> <h1 id="grokfast---effect-of-network-width">GrokFast - effect of network width</h1> <p>Next, I want to test the effect of network width on GrokFast performance, with the setup of 3 layers of network.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/aassets/img/grok_fast/grokfast_128p.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> at around $$10^4$$ steps, the test acc experienced a decrease and then slowly increase at around 5*10^4 steps </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/grok_fast/grokfast_512p.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> at around $$10^4$$ steps, the test acc experienced a slight decrease and then rapidly increase. </div> <p>Based on the observation above, I conclude that network width can help network regain its val accuracy after the mysterious drop in val acc.</p> <h1 id="combination-with-lora">Combination with LoRA</h1> <p>Due to the phenomenon, I planed to use Grokfast on LoRA. Specifically, using the gradient update code on matrix A and matrix B can work maybe. However, the first test result was confusing.</p> <p>The first setup is traditional 3 layer + 200 hidden size. GrokFast implemented MA optimizer, and here’s the result</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/grok_fast/grokfast+LoRA.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>While val acc and train acc climbed up together at around 10^4, val acc suddenly dropped and continued to decrease until the end of training time.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/aassets/img/grok_fast/grokfast_ema.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For ema optimizer, the val acc sharply increase at \(10^4\) steps and then increases at a very slow pace.</p>]]></content><author><name></name></author><category term="encoding"/><category term="method"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[As mentioned in previous blog, grokking is a phenomenon that extremely long training time leads to a sharp incease in val accuracy. Some researchers try to find a way to speed up the grokking phenomenon. GrokFast proposed a new optimizer to boost slow-varying component of the gradient as they hypothesized that it was a contributing factor to Grokking phenomenon.]]></summary></entry><entry><title type="html">History of Position Encoding</title><link href="https://kasjfksj.github.io/blog/2024/position-encoding/" rel="alternate" type="text/html" title="History of Position Encoding"/><published>2024-08-22T09:15:09-07:00</published><updated>2024-08-22T09:15:09-07:00</updated><id>https://kasjfksj.github.io/blog/2024/position-encoding</id><content type="html" xml:base="https://kasjfksj.github.io/blog/2024/position-encoding/"><![CDATA[<h2 id="background-positional-information">Background-Positional information</h2> <p>Compared with other sequential model like LSTM and RNN, Transformers are much more successful and serve as the foundation of current language models. These transformer-based LLM achieve higher accuracy than other architecture and can scale up easily due to the parallel computing of Transformer. However, Transformers have its flaws. It requires more computation resources than Sequential model with time complexity of \(O (N^2)\). Also, without external help, Transformers can’t acquire positional information that is important when dealing with sequential data.</p> <p>What’s positional information? Positional information is about the order of characters appear. For instance, “I eat fish” is different from “Fish eat I” because order of appearance is different. “I” appears first in the first setence and “fish” appears first in the second sentence, resulting each sentence having different meaning.</p> <p>Transformers can’t acquire the sequential information of words because the way data is passed to the model. For a traditional model like RNN, each data is passed one at a time. For instance, in the sentence “I eat fish,” “I” will be processed first, then “eat”, then “fish.” This way, RNN acquires sequential information of each word.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/recurrent_network.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>However, for transformers, all words are passed in all at once. The sequential meaning is lost when the model processes data parallelly instead of sequentially. All words in “I eat fish” will be passed into the model at the same time. The model can’t distinquish which words come first, unless we pass in additional positional information about this word.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/Transformer.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Of course, we can just simply assign 1, 2, 3, etc to each word position, but researchers found out better ways to encode position of words.</p> <h2 id="absolute-positional-encoding---sinusoidal-positional-encoding">Absolute Positional Encoding - Sinusoidal Positional encoding</h2> <p>Absolute positioinal encoding is first introduced in the paper Attention Is All You Need. It uses trigonometry function, sine and cosine, to encode positions.</p> <p>For a word \(x\) at position t, the positional encoding will be expressed as:</p> \[e_t = \begin{bmatrix} sin(w_0 \cdot t) \\ cos(w_0 \cdot t) \\ sin(w_1 \cdot t) \\ cos(w_1 \cdot t) \\ \vdots \\ sin(w_{d/2} \cdot t) \\ cos(w_{d/2} \cdot t) \end{bmatrix} \ \ w_k = \frac{1}{10000^{\frac{2k}{d}}} \ \ where \ \ d \ \ is \ \ the \ \ dimension \ \ of \ \ the \ \ positional \ \ embedding\] <p>Using trigonometry function has a very good property. Given word \(p_i\) at position i and word \(e_{i+k}\) at position i+k, we can deduce the position between them by taking dot product.</p> \[\begin{split} e_t \cdot e_{t+k} &amp; = \sum_{i=0}^{\frac{d}{2}-1}sin(w_it)sin(w_i(t+k))+cos(w_it)cos(w_i(t+k)) \\ &amp; = \sum_{i=0}^{\frac{d}{2}-1} cos(w_i(t-(t+k))) \\ &amp; = \sum_{i=0}^{\frac{d}{2}-1} cos(-w_ik) \end{split}\] <p>We can see that the final result is only dependent on k, the relative distance between each word.</p> <p>Let’s recall how attention, the main mechanism of Transformer, works with KQV stuffs.</p> \[Attention(Q,K,V) = softmax(\textbf{QK}^T)\textbf{V}\] \[\textbf{Q} = \textbf{W}_Qx\] \[\textbf{K} = \textbf{W}_Kx\] \[\textbf{V} = \textbf{W}_Vx\] <p>We may only focus on \(\textbf{QK}^T\) because that’s where two words interact each other. We now inject sinusoidal positional encoding to input vector, so instead of \(x\), it’s \(x+e\). We then calculate attention score for words in position i and j, the result will be:</p> <p>\(\textbf{W}_Q(x_i+e_i)(\textbf{W}_K(x_j+e_j))^T =\) \(\textbf{W}_Qx_i{x_j}^T\textbf{W}_K+\textbf{W}_Qe_i{x_j}^T\textbf{W}_K+\textbf{W}_Qx_i{e_j}^T\textbf{W}_K+\textbf{W}_Qe_i{e_j}^T\textbf{W}_K\)</p> <p>We can see that the first term doesn’t contain any positional information of two words. The second and third term only contain positional information of 1 word, which alone can’t deduce the relative positional information. Only the fourth one that has the dot product of two positional embedding contain relative position of two words. This piece of information will be helpful to the model to identify the revelance between each words.</p> <p>Questions:</p> <ol> <li>Why do we add positional embedding to input embedding? What about concatenation? a. It may blur semantic meaning of input embedding by interwining positional embedding with semantic content b. It’ll increase dimension of input vector with no significant increase of model’s accuracy</li> <li>Why many people call sinusoidal positional encoding as absolute? They say it can’t learn relative positional information. a. I searched online looking for answers why this encoding method is absolute and can’t acquire distance between words. Sadly, all the blog I have checked so far didn’t contain detailed mathematical formula to prove it. I guess they call it absolute simply because it only takes the input of current word position and no other word positions.</li> </ol> <h2 id="conclusion">Conclusion</h2> <p>Positional encoding is one of the most important feature in Transformer. This blog covers one of the encoding method, the Absolute Positional encoding. Many people say about this absolute positional encoding can’t acquire relative distance two words because it’s “absolute.” However, I do some mathematical deduction and show that this encoding method can learn the relative distance between two words. Therefore, it’s useful in acquiring information beside the current position of the word.</p>]]></content><author><name></name></author><category term="encoding"/><category term="method"/><summary type="html"><![CDATA[Background-Positional information]]></summary></entry><entry><title type="html">Multimodality Architecture</title><link href="https://kasjfksj.github.io/blog/2024/Multimodality-architecture/" rel="alternate" type="text/html" title="Multimodality Architecture"/><published>2024-08-21T00:00:00-07:00</published><updated>2024-08-21T00:00:00-07:00</updated><id>https://kasjfksj.github.io/blog/2024/Multimodality-architecture</id><content type="html" xml:base="https://kasjfksj.github.io/blog/2024/Multimodality-architecture/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">LoRA - a potential parameter efficient fine-tuning method for large models</title><link href="https://kasjfksj.github.io/blog/2024/LoRAa/" rel="alternate" type="text/html" title="LoRA - a potential parameter efficient fine-tuning method for large models"/><published>2024-08-13T09:15:09-07:00</published><updated>2024-08-13T09:15:09-07:00</updated><id>https://kasjfksj.github.io/blog/2024/LoRAa</id><content type="html" xml:base="https://kasjfksj.github.io/blog/2024/LoRAa/"><![CDATA[<h2 id="background">Background</h2> <p>Imagine you are a researcher testing out language models such as Small T5 or LSTM and you have decent but not great amouont of GPUs. You want to finetune the model to test their performances on a few datasets. You train your model on the datasets and get the results. You are satisfied.</p> <p>But what if there are hundreds of datasets you want to test and not only that, you want to test out current LLM like LLama and Mistral that have billions of parameters. It’s impossible to train on your limited GPU since there are so many parameters that need to be updated through back propogation. You need a way to minimize the number of parameter updates without costing too much computation resources. This is where LoRA comes to play.</p> <h2 id="gradient-descent">Gradient Descent</h2> <p>Every AI model, LLM or CNN, is basically a mathematical function that’s based on some parameters \(\theta\). Parameters are usually weight matrix that multiply with input matrix. For most datasets, we have inputs \(X\) and targets \(Y\). For instance, the input can be a sentence and the output is 0 and 1 where 0 represents negative sentiment and 1 represents positive sentiment. We give the model input \(x \ \ and \ \ x \in X\), and it outputs \(\textit{f}(x, \theta)\). However, we wish that the output will be the target \(y \ \ and \ \ y \in Y\). We use gradient descent to minimize the distance between outcomes and desired outcomes.</p> \[W_t = W_{t-1} + \Delta W \ \ where\ \ W\ \ is \ \ parameter \ \ and \ \ t \ \ is \ \ each \ \ iteration\] <p>For the training session, most computation lies in computing gradient, notably matrix multiplication, which will take incredibly long time when the model size scales up. We can handle this amount of computation on a few datasets, but it’ll be too much when there are many datasets to fine tune on.</p> <h2 id="lora">LoRA</h2> <p><a href="https://arxiv.org/abs/2106.09685">LoRA</a> solves this problems by assuming that the ‘change in weights during model adaptation also has a low “intrinsic rank”’, meaning that the updated parameter can be expressed as the multiplication of 2 low rank matrix. “Rank” is a term in Linear Algebra, which is equivalent to new information. Higher the rank the matrix has, more information the matrix contains.</p> <p>The update formula in LoRA is written as follow:</p> \[W = W_0 + AB \ \ where \ \ A\in R^{m\times r} \ \ and \ \ B\in R^{r\times n}. \ \ Here \ \ r\ll m \ \ and \ \ n\] <p>This way, the computation for gradient will be greatly reduced. For instance, suppose the weight matrix \(W\) is 768 by 1024 matrix. When we calculate the gradient for this matrix, we need to compute 768 \(\times\) 1024 parameters. For \(A\) and \(B\) matrix, we can let A be a 768 by 32 matrix and B be a 32 by 1024 matrix. When the model is doing gradient descent, we froze the model’s parameter, so the gradient for the \(W\) is 0. We only need to calculate gradient for \(A\) and \(B\), which has much less parameters than \(W\). Thus, the computation cost reduces significantly.</p> <h2 id="relora">ReLoRA</h2> <p>Currently, LoRA and its variants, QLoRA, MLoRA, etc, focus on fine-tuning models that are fully trained on datasets. Although during the fine-tuning stage, they require less computations, when we take previously fully trained model into account, it still takes quite a lot of computations. Can we apply LoRA as a training method to a model instead of fully training?</p> <p>When we look at the equation in LoRA closely, we can move \(W_0\) to the left and obtain:</p> \[\Delta W = AB \ \ where \ \ \Delta W = W - W_0\] <p>In a way, we can treat the \(AB\) as the gradient of the model, and yes, we can use LoRA to train a model. This idea is explored by ReLoRA.</p> <p>In their paper, they use LoRA as gradient to update model parameter. First, they train the model just like LoRA without updating model’s parameters. After 2000 steps, the matrix A and B will multiply and add to the model’s weight. Then both matrix will be reinitialize and get trained again.</p> <p>There are several advantage of this method. Firstly, this is much more parameter-efficient than fully-trained model. Secondly, according to the paper, ReLoRA outperforms LoRA though still can’t compare with fully trained model. Thus, the idea of using low-rank update for high rank matrix does work.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/35001724304205_.pic.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="conclusion">Conclusion</h2> <p>LoRA is one of the most popular PEFT method for its simplicity and effectiveness. From my intuition, the way LoRA update parameters is similar to how human learns new things. We often uses previous knowledge and updates the knowledge with some adjustment. For instance, we draw inspiration from real numbers and apply the same arithmetic operations on imaginary number with additional rule that \(i \times i = -1\). I believe LoRA is possibly the key component towards continual learning, making the model more knowledgable about the world and therefore, become the world model.</p>]]></content><author><name></name></author><category term="fine-tune"/><summary type="html"><![CDATA[Background]]></summary></entry><entry><title type="html">Summer class experience</title><link href="https://kasjfksj.github.io/blog/2024/UCB-summer/" rel="alternate" type="text/html" title="Summer class experience"/><published>2024-08-09T09:15:09-07:00</published><updated>2024-08-09T09:15:09-07:00</updated><id>https://kasjfksj.github.io/blog/2024/UCB-summer</id><content type="html" xml:base="https://kasjfksj.github.io/blog/2024/UCB-summer/"><![CDATA[<p>This summer, I took UC Berkeley’s CS 61A course which was hard as the rumor said, but my journey included more than just academic challenges. It began with a housing issue. I had arranged to stay in a UCB student house, but just before my arrival, the landlord cut my stay short to July 31 instead of August 15, which meant I had to move out before even finishing my course. Moreover, I was worried it might be a scam, so I quickly canceled and had to scramble for new accommodation.</p> <p>With no place to stay, I had to stay in hotel for 6 days, with sales at $99 a night. It was really expensive. I tried to search for a cheaper option. Fortunately, another CS 61A student, W, offered me a room. To my surprise, he was also a UC Berkeley student, which was a huge benefit since he could give me insights into campus life, dining spots, and local tips.</p> <p>As I began my summer course in UC Berkeley’s CS 61A, I felt confident. The early lectures covered basic Python concepts like booleans, while loops, and functions—topics I knew well. The homework and projects were manageable, and I finished them early. I was optimistic about getting an A+.</p> <p>During this time, I explored the campus with W. It was vast and impressive, almost like a small town with magnificient buildings. Since the campus is on a slope, walking around was tiring. I asked W why the university was built on a hill. Why wouldn’t they build on plains? He shrugged and said when people constructed the campus, this hill was probably the only available place for them to build. We also visited some iconic buildings and sculptures donated by alumni decades ago. This whole place is magnificent and historical, which was really amazing. We also checked out local restaurants, dessert store, and stores like Target and Safeway.</p> <p>Everyday, I would get up, shortly prepare breakfast and get dressed, and take No. 88 or 36 bus with W . Before the lecture, I’d visit a small café in the underground area run by a friendly old lady. I would usually order chocolate chip cookies or muffins as snacks during class. Because I went there so often, that lady reckonized me and would prepare the snacks before I even ordered. It was really kind and considerate of her, remembering me and my preference.</p> <p>After the one and a half hours of lecture, we would head downhill to the Subway, where I would get the BEAST. Although it was really salty, probably from the chess, I really liked the mix of sausage, beef, lettuce. It was relatively cheap compared with others. The crunchiness of lettuce and bread, softness of meat and cheese, the meaty flavor and freshness from vegetables all entangled in my mouse, creating a symphony of taste.</p> <p>After that, we would go to the lab or discussion, depending on the day. In fact, there were no real discussions. Most lab and discussion were just about doing homework and projects, which I could easily get bored when I finished my work. After this period finished, it was in the mid afternoon, where we often had relaxation nearby. I would go to the gelato store near campus, and my favorite flavor is tiramisu or, if the store didn’t provide the flavor, coffee. I would just sit outside, eating the dense and rich-flavored ice cream, and watching cars and crowds pass by. Sometimes, we would go to the gym to exercise. W was more into developing his muscle while I just want to keep myself fit, so the climbing and running machine would be my favorite. The above was perhaps the summary of daily life.</p> <p>During my stay with him, W shared his experience with UC Berkeley. He said to me that he majored in History and Political economy, which startled me very much. I had no idea why he would take computer science when history and politics had nothing to do with STEM. He first told me in a tone like an old professor, saying that every fields were related to each other, even though the combination would seem ridulous, but still existed. Of course, he then said, the reason is to learn new skills, especially when social science required data collection and processing. The programming skill would potentially help him get a job. However, UCB was such a top-notch in STEM and its STEM courses, from W’s articulation, were horrendously hard. He had taken CS 61A for 2 times but had to drop from it because he did so bad that it could greatly affect his GPA. I told him I had a really good foundation on Python. He seemed greatly relieved. I was skeptical about his statement at first. Sure, UCB course was challenging, but how hard could CS 61A, an intro course, be?</p> <p>As the course continued, I began to see what W meant about its difficulty. Topics like iterators and generators were completely new to me, and I needed to watch videos to understand them. The sample exams was the most daunting; even with the answers, some questions were confusing. I realized this course was very tough and that I needed to be very careful to get an A. I started preparing for the midterm two weeks in advance, working on exam questions and reviewing all my notes. I began attending tutoring sessions to augment my understanding. The stress increased, and there was even a period that I questioned if computer science was right for me. I might as well just quit the course, I thought, perhaps only UCI courses fit me.</p> <p>Fortunately, I had excellent TAs, tutors, and lecturers. They answered my questions patiently and their encouragement helped me cope with the stress. One tutor inspired me the most. He was a sophomore who had never programmed before taking CS 61A. He practiced past exams multiple times and got his exam score from 50 to 60 and then finally to 90. His hard work motivated me and I believed that I needed to have the same dedication as him and I kept more practicing. Even W was amazed by my dedication when he asked me why I studied hard for this course even though I had relatively good foundation on programming. I told him I wanted to get A+ on this course and also told him about the adamant spirit of that tutor, and he was also inspired to study past exam together with me.</p> <p>Midterm day finally arrived, and I felt restless as soon as I woke up. The exam was scheduled from 7:00 to 9:00 p.m., so I had plenty of time to review. Despite this, I struggled to focus during my practice exams because I was so anxious about the real test. I told W that I would head to school early and walk around. To calm my nerves, I had some Italian spaghetti for lunch and gelato for dessert. I spent most of the time sitting on the grass land, enjoying the warm breeze and cool grass.</p> <p>As the exam time approached, I went to the assigned classroom. The exam started at 7:03 p.m. I felt prepared for the first question because I had done well on first questions in past exam. But when I flipped the page, my heart nearly stopped. Instead of the expected high-order functions or environment diagrams, the first question was about generators and iterators—topics I had just recently learned. The next page had a question about list operations, another difficult topic for me. I panicked for a few minutes at first, trying to figure out the answers, but then I decided to move on to questions I was more confident about. Most questions were fine, and I could understand all the recursion questions. After finishing those, I went back to previous questions, struggling to remember everything about iterators, generators, and list operations. I spent an hour on those two questions but was still unsure about my answers after the exam ended. I felt devastated by the difficulty and told W, “It’s over. I don’t think I’ll get an A. I might just aim for a B.” Just two days later, W told me our exam scores were out. I hesitated to check, fearing a low score. When I finally opened the email, I was shocked to see that I had scored 61.5 out of 64. At first, I thought it might be a mistake since I did poorly on the first two questions. However, I learned that the class did so poorly that the lecturers had to curve the whole class. I was amazed and utterly happy about my result because it meant that I could have 20 points reduction on the final exam, which meant I have more change to get an A+ in the exam.</p> <p>After the midterm, I felt less stressed about the UCB course and had more time to enjoy life. I met a girl who was from community college in San Jose. Every day, she woke up early to catch a train to UCB. She struggled with her classes and stayed at UCB until 9 p.m. for extra tutoring. She complaint to us about facing harassment during her commute, which worried us. We suggested she try to return home earlier, but she said the TAs only posted their schedules in person, not online. We decided to ask the TAs if they could offer online tutoring for her. They agreed, which helped her avoid returning home late. She was grateful and started spending more time with us.</p> <p>As we got to know her, I learned that she was from mainland China, like W and me. We formed a small study group to help her with programming. She was not good at programming compared with W. However, she was hard-working and patient in learning new knowledge. She would repeatedly ask us questions until she fully understood the concept.</p> <p>One time, we enjoyed lunches together at an Italian restaurant. She told us about her background. Her father owned two hotels, providing her family with a good income. She had even thought about inheriting the family business.</p> <p>“Then why did you come to America instead of China?” W asked.</p> <p>The girl then explained, sadly, about COVID-19 pandemic that had hurt her family’s business, causing financial problems. She realized she couldn’t depend on her family and needed to find her own way. That’s why she came to the U.S., hoping to work in the quantization industry. She took CS 61A to gain programming skills that would help her stand out in job applications. “Every field needs CS skills these days,” W said. “Yep, that’s the trend,” I agreed.</p> <p>Soon, the final exam day arrived. W and I decided to get to campus early and I went to the cafe downstairs with W. W had rarely been here and was immediately attracted to a coin jar for donations. The jar had coins from different countries glued to it—Israel, Japan, China, Sweden. An older lady explained that she had been collecting coins from tourists for years, just for fun. We examined each coin with amazement, noting the unique designs that represented different cultures. I was impressed by W’s observation since I had ordered about 20 brownies and hadn’t even noticed the jar.</p> <p>The final exam started at 7 p.m. It turned out to be relatively easy, focusing mainly on Scheme and SQL. Although these were new topics for me, they felt familiar after hours of practices.</p> <p>After finishing final exam, stepping out of the classroom into the crisp evening air at 10 p.m., I felt a deep sense of achievement from completing the challenging course. After 3 days, I got an email saying I scored 89 out of 92, meaning I could get an A for sure, which greatly relieved me. W initially had A-, but his final score was so close to the minimum requirement of an A. He requested a regrade on the exam, and sure enough, the lecturers gave some points on his final exam, and he finally, for the first time, got an A in CS 61A. He was greatly motivated by this and told me that he wanted to take additional CS course like 61B. “Go ahead,” I encouraged. The girl, however, didn’t perform well on the exam and ultimately got a B-. It was nevertheless her first time to code, and she thanked us for helping her out because without our help, she might as well get a C- or even worse.</p> <p>Overall, it had been a tough but incredible experience, unlike anything I had encountered at UCI. I made new friends, traveled to another city, and lived there for two months. More importantly, I gained confidence in tackling difficult challenges, knowing that my positivity could help me overcome any obstacles.</p> <p>Yet, at the night before I left, I couldn’t fell asleep when I realized that I had to say goodbye to everything I had enjoyed: the campus, the course, the TAs, the Italian restaurants, the gelato store, Safeway, and the buses. I would miss them very much after leaving.</p> <p>After I arrived irvine, I got the letter that notified me that I got an A+. I smiled and knew I had achieved my goals.</p>]]></content><author><name></name></author><category term="classes"/><category term="experience"/><summary type="html"><![CDATA[This summer, I took UC Berkeley’s CS 61A course which was hard as the rumor said, but my journey included more than just academic challenges. It began with a housing issue. I had arranged to stay in a UCB student house, but just before my arrival, the landlord cut my stay short to July 31 instead of August 15, which meant I had to move out before even finishing my course. Moreover, I was worried it might be a scam, so I quickly canceled and had to scramble for new accommodation.]]></summary></entry></feed>