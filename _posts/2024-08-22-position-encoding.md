---
layout: post
title: History of Position Encoding
date: 2024-08-21 16:15:09
description: 
tags: formatting images
categories: encoding method
tabs: true
---

### Background

Compared with other sequential model like LSTM and RNN, Transformers are much more successful and serve as the foundation of current language models. These transformer-based LLM achieve higher accuracy than other architecture and can scale up easily, . However, Transformers in that it lacks of positional information.

### Positional Information

What's positional information? 
